{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7430e9ed-9ccc-43c1-bc29-74b8c120a4e5",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 4: Predicting Presence of West Nile Virus <br>\n",
    "**Notebook 4b: Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f4137-218e-44a6-bfc1-3ce6c7d735c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TABLE OF CONTENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5444a3b-7110-4cff-9c09-5859a339fafc",
   "metadata": {},
   "source": [
    "**1a. EDA on Training Dataset** <br>\n",
    "**1b. EDA on Weather Dataset** <br>\n",
    "**1c. EDA on Spray Dataset** <br>\n",
    "**2. Data Preprocessing I** <br>\n",
    "**3. Data Preprocessing II** <br>\n",
    "**4a. Modelling (Pycaret)** <br>\n",
    "**4b. Modelling (This Notebook)** <br>\n",
    "- [(1) HyperParameter Tuning](#(1)-Gridsearch-for-Logistics-Regression,-Gradient-Boosting-Classfier,-and-Light-Gradient-Boosting-Machine-Models.) <br>\n",
    "- [(2) Choosing a Production Model](#(2)-Choosing-a-Production-Model) <br>\n",
    "- [(3) Examination of Most Important Variables](#(3)-Examination-of-Most-Important-Variables)\n",
    "- [(4) Further Exploration](#(4)-Further-Exploration:-The-Case-for-Higher-Rolling-Days)\n",
    "- [(5) Bonus: Logistics Regression with SMOTE](#(5)-Bonus:-Logistics-Regression-with-SMOTE)\n",
    "- [(6) Bonus: Neural Nets Implementation](#(6)-Bonus:-Neural-Nets-Implementation)\n",
    "\n",
    "**5. Cost Benefit Analysis** <br>\n",
    "**6. Conclusion & Recommendations** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e101a1-31e2-414c-ba27-131846ef31a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For distance measuring given lat long\n",
    "import geopy.distance\n",
    "\n",
    "# for apply progress tracking\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import get_scorer, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, plot_roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca7aa24-3dd3-4218-a5bc-aecd0b33bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we switch off the warnings as not all models being grid-searched converges\n",
    "# Nevertheless, this is ok as the gridsearch will ultimately default to / compare models that converge\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5987506-47b8-483d-b9cc-301a3ea8fde6",
   "metadata": {},
   "source": [
    "# Modelling Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3459f-e301-469c-acda-e488f2246ed4",
   "metadata": {},
   "source": [
    "This section build-ups on [a broad scan of the classifier methods available using PyCaret.](./4a_Model_Scan_Pycaret.ipynb) Using AUC as a scoring criteria with CV as a validation method, we ascertained that Logistics Regression, Gradient Boosting Classifier and the Light Gradient Boosting Machine would be suitable methods to further develop our model upon. This section will see us conducting a gridsearch to optimise models built using said classifier methods, towards identifying a final production model for our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b300a4d6-abfe-4b5b-8621-207bd1d244b0",
   "metadata": {},
   "source": [
    "## (1) Gridsearch for Logistics Regression, Gradient Boosting Classifier, and Light Gradient Boosting Machine Models.\n",
    "\n",
    "Alongside identifying the best model for our case, we also seek to determine the optimal number of rolling days average to use for our weather-related variables. The use of such rolling days average reflects our theory about mosquito activity - that mosquito activity on a given day is not merely a function of the weather conditions on the day itself. \n",
    "\n",
    "Rather, peak mosquito activity arises as the culmination of hot and humid conditions over several days. This \"number of days\" accounts for (1) the duration of optimal weather required to \"awake\" and hatch the mosquito eggs from their wintry hibernation, (2) the number of days required for a newly-hatched mosquito larvae to fully mature into an adult mosquito, and (3) the presence of \"good-weather\" days for an adult mosquito to become fully active and be on the prowl for both mates and blood. \n",
    "\n",
    "For modelling purposes, we will use rolling averages of 10 days, 20 days, and 30 days in our models (a baseline model employing 1 rolling day will also be employed to validate our theory about mosquito activity). The best performing model will be used for our final production model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d0925-f2c9-44da-9fba-3f58cfc74f9a",
   "metadata": {},
   "source": [
    "### (a) Logistics Regression Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7911e54b-1783-4a39-ba2b-34cbbbce3111",
   "metadata": {},
   "source": [
    "In our logistics regression modelling, we used the \"saga\" solver and Ridge Regression exclusively as we found it to be the only solver-penalty combo that achieved convergence (sidenote: the \"saga\" solver was purpose-built for large dataset such as this). We also stratified our dataset based on our predicted variable for our train test data split, and set our class weights to \"balanced\" to accommodate the highly-imbalanced nature (~5% positive predictions) of our dataset. Lastly, we adopted Standard scaling for our weather-related variables, for both quicker convergence as well as the understanding that the large volume of weather could fit onto a standard scale. We further reduce the tolerance for convergence to 0.005 to accelerate the convergence process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335c9b28-941b-4e4a-af2e-03fe5eea2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_model(rolling_days, drop_codesum = False):\n",
    "    \n",
    "    # load in data\n",
    "    locals()['train_' + str(rolling_days)] = pd.read_csv('./assets/Modelling_Data/train_r'+ str(rolling_days) +'.csv', index_col=0)\n",
    "    \n",
    "    # drop Date Column\n",
    "    locals()['train_' + str(rolling_days)].drop(columns = ['Date'], inplace = True)\n",
    "    \n",
    "    # drop CodeSum Column\n",
    "    if drop_codesum == True:\n",
    "        locals()['train_' + str(rolling_days)].drop(columns = ['MIFG','TS','SQ','GR','VCFG','FG+','SN','FG',\n",
    "                                                               'VCTS','BCFG','BR','RA','FU','DZ','TSRA','HZ'], inplace = True)\n",
    "    \n",
    "    # Create X and Y\n",
    "    X = eval('train_' + str(rolling_days)).drop(columns = ['WnvPresent'])\n",
    "    X.reset_index(drop = True, inplace = True)\n",
    "    y = eval('train_' + str(rolling_days))['WnvPresent']\n",
    "    \n",
    "    # Dummify Columns\n",
    "    X = pd.get_dummies(X, columns=['Species', 'Trap'], drop_first = True)\n",
    "    \n",
    "    # Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 88, stratify = y)\n",
    "    \n",
    "    #Standard Scale weather variables\n",
    "    ct = ColumnTransformer([(\"sc\", StandardScaler(), \n",
    "                             ['Tavg', 'PrecipTotal','StnPressure','ResultDir','AvgSpeed','Sunlight'])],\n",
    "                          remainder = 'passthrough')\n",
    "\n",
    "    X_train_sc = ct.fit_transform(X_train)\n",
    "    X_test_sc = ct.transform(X_test)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    X_train_sc = pd.DataFrame(X_train_sc, columns=X_train.columns)\n",
    "    X_test_sc = pd.DataFrame(X_test_sc, columns=X_test.columns)\n",
    "    \n",
    "    # Instantiate a pipeline\n",
    "    pipe_logreg = Pipeline([\n",
    "        ('logreg', LogisticRegression(tol = 0.005, class_weight='balanced', solver = 'saga', max_iter=10000, random_state = 888)) # tuple for estimator object, class\n",
    "    ])\n",
    "    \n",
    "    # Input Search Parameters!\n",
    "    pipe_logreg_params = {'logreg__C':np.logspace(1,3,2)}\n",
    "    \n",
    "    # Instantiate a Grid Search\n",
    "    gs_logreg = GridSearchCV(pipe_logreg, \n",
    "                             param_grid=pipe_logreg_params,\n",
    "                             cv=5,\n",
    "                             verbose=1,\n",
    "                             n_jobs=-1\n",
    "                            )\n",
    "\n",
    "    # Score!\n",
    "    gs_logreg.fit(X_train_sc, y_train) # 1. fit model on training data first\n",
    "\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']: # define a list of metrics we want computed on test\n",
    "        print(f'Train {metric}: {get_scorer(metric)(gs_logreg, X_train_sc, y_train)}') # pass to get_scorer for metrics computation\n",
    "    print(f\"Train AUC = {get_scorer('roc_auc')(gs_logreg, X_train_sc, y_train)}\")\n",
    "\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']: # define a list of metrics we want computed on test\n",
    "        print(f'Test {metric}: {get_scorer(metric)(gs_logreg, X_test_sc, y_test)}') # pass to get_scorer for metrics computation\n",
    "    print(f\"Test AUC = {get_scorer('roc_auc')(gs_logreg, X_test_sc, y_test)}\")\n",
    "    \n",
    "    return(gs_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab57dfa-570b-4edd-9ee8-3c3b56c6c66a",
   "metadata": {},
   "source": [
    "#### Logistics Regression Across Different Rolling Days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c385ae-d8cf-4b3d-b5b2-61a41363cc89",
   "metadata": {},
   "source": [
    "**(i) 10 Rolling Days Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2806a389-4645-47f7-b18b-cba9c2c264f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Train accuracy: 0.729507032248899\n",
      "Train precision: 0.14352066883418485\n",
      "Train recall: 0.8373983739837398\n",
      "Train f1: 0.24504361617763679\n",
      "Train AUC = 0.8530163779898668\n",
      "Test accuracy: 0.7216613787135853\n",
      "Test precision: 0.12607449856733524\n",
      "Test recall: 0.7252747252747253\n",
      "Test f1: 0.21480878763222133\n",
      "Test AUC = 0.7966154849716494\n"
     ]
    }
   ],
   "source": [
    "logreg_r10 = log_reg_model(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2cf19d-36c5-4287-8280-51dd5864ff34",
   "metadata": {},
   "source": [
    "**(ii) 20 Rolling Days Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7256e00-e20a-4c6f-ad9d-b3dfbe3b75e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Train accuracy: 0.7383151015769286\n",
      "Train precision: 0.14844868735083533\n",
      "Train recall: 0.8428184281842819\n",
      "Train f1: 0.25243506493506496\n",
      "Train AUC = 0.8606219248099528\n",
      "Test accuracy: 0.7288722238246322\n",
      "Test precision: 0.13275193798449614\n",
      "Test recall: 0.7527472527472527\n",
      "Test f1: 0.22570016474464583\n",
      "Test AUC = 0.817625068994932\n"
     ]
    }
   ],
   "source": [
    "logreg_r20 = log_reg_model(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9c90f-072b-47bc-8b45-9d4423a61206",
   "metadata": {},
   "source": [
    "**(iii) 30 Rolling Day Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67bda983-b4cc-4608-a6a9-3681e96e5a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Train accuracy: 0.7459866458303737\n",
      "Train precision: 0.1530562347188264\n",
      "Train recall: 0.8482384823848238\n",
      "Train f1: 0.2593206296603149\n",
      "Train AUC = 0.8667595064256489\n",
      "Test accuracy: 0.748774156331122\n",
      "Test precision: 0.14666666666666667\n",
      "Test recall: 0.7857142857142857\n",
      "Test f1: 0.24719101123595508\n",
      "Test AUC = 0.8267507986686069\n"
     ]
    }
   ],
   "source": [
    "logreg_r30 = log_reg_model(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848dfb9-8723-4f0c-8f9f-f047de71ee54",
   "metadata": {},
   "source": [
    "**(iv) Baseline Model: Single Day Rolling Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4327e706-eb2f-4dad-acda-2340a13802ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Train accuracy: 0.677368944452337\n",
      "Train precision: 0.11497975708502024\n",
      "Train recall: 0.7696476964769647\n",
      "Train f1: 0.20007044734061288\n",
      "Train AUC = 0.8052638314988847\n",
      "Test accuracy: 0.6795500432650706\n",
      "Test precision: 0.11323896752706078\n",
      "Test recall: 0.7472527472527473\n",
      "Test f1: 0.19667389732465654\n",
      "Test AUC = 0.7634410490574874\n"
     ]
    }
   ],
   "source": [
    "logreg_r1 = log_reg_model(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a290d-8735-47ff-a736-268b6cbe9df3",
   "metadata": {},
   "source": [
    "**We note the following:**\n",
    "\n",
    "(1) That the **logistic regression model achieved respectable performance, with the best performing model using a 30 days rolling average (0.867 Train AUC and 0.827 Test AUC)**. \n",
    "\n",
    "(2) That all models employing a rolling average for our weather-related parameters performed vastly more superior than the model using only \"actual day\" (i.e. rolling day of 1) weather conditions. This **validates our theory that peak mosquito activity is the culmination of conducive weather over many days, and not just a single day**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2359f824-099b-41ec-acf8-3d9883975ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg__C': 1000.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Examination of best parameters\n",
    "logreg_r30.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7726b2a-49dd-4b56-bac6-022688d897f7",
   "metadata": {},
   "source": [
    "### (b) Gradient Boosting Classifier Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03067a1a-975d-4688-b740-0c239a4d5724",
   "metadata": {},
   "source": [
    "We model using the Gradient Boosting Classifier next, conducting a gridsearch over different depth, number of estimators, and learning rates. Likewise, we adopted measures to accommodate the highly imbalanced dataset, as well as Standard scaling for our weather-related variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a127ff-0236-4554-b4ef-2541622fdbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_boost_model(rolling_days, drop_codesum = False):\n",
    "    \n",
    "    # load in data\n",
    "    locals()['train_' + str(rolling_days)] = pd.read_csv('./assets/Modelling_Data/train_r'+ str(rolling_days) +'.csv', index_col=0)\n",
    "    \n",
    "    # drop Date Column\n",
    "    locals()['train_' + str(rolling_days)].drop(columns = ['Date'], inplace = True)\n",
    "    \n",
    "    # drop CodeSum Column\n",
    "    if drop_codesum == True:\n",
    "        locals()['train_' + str(rolling_days)].drop(columns = ['MIFG','TS','SQ','GR','VCFG','FG+','SN','FG',\n",
    "                                                               'VCTS','BCFG','BR','RA','FU','DZ','TSRA','HZ'], inplace = True)\n",
    "    \n",
    "    # Create X and Y\n",
    "    X = eval('train_' + str(rolling_days)).drop(columns = ['WnvPresent'])\n",
    "    X.reset_index(drop = True, inplace = True)\n",
    "    y = eval('train_' + str(rolling_days))['WnvPresent']\n",
    "    \n",
    "    # Dummify Columns\n",
    "    X = pd.get_dummies(X, columns=['Species', 'Trap'], drop_first = True)\n",
    "    \n",
    "    # Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 88, stratify = y)\n",
    "    \n",
    "    #Standard Scale weather variables\n",
    "    ct = ColumnTransformer([(\"sc\", StandardScaler(), \n",
    "                             ['Tavg', 'PrecipTotal','StnPressure','ResultDir','AvgSpeed','Sunlight'])],\n",
    "                          remainder = 'passthrough')\n",
    "\n",
    "    X_train_sc = ct.fit_transform(X_train)\n",
    "    X_test_sc = ct.transform(X_test)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    X_train_sc = pd.DataFrame(X_train_sc, columns=X_train.columns)\n",
    "    X_test_sc = pd.DataFrame(X_test_sc, columns=X_test.columns)\n",
    "    \n",
    "        \n",
    "    # Instantiate a pipeline\n",
    "    pipe_gradboost = Pipeline([\n",
    "        ('gradboost', GradientBoostingClassifier(random_state = 88)) # tuple for estimator object, class\n",
    "    ])\n",
    "    \n",
    "    # Input Search Parameters!\n",
    "    pipe_gradboost_params = {'gradboost__max_depth': [3,4],\n",
    "                             'gradboost__n_estimators': [50, 75],\n",
    "                             'gradboost__learning_rate': [.08, .1, .12]}\n",
    "    \n",
    "    # Instantiate a Grid Search\n",
    "    gs_gradboost = GridSearchCV(pipe_gradboost, \n",
    "                                param_grid=pipe_gradboost_params,\n",
    "                                cv=5,\n",
    "                                verbose=1,\n",
    "                                n_jobs=-1\n",
    "                               )\n",
    "\n",
    "    # Score!\n",
    "    gs_gradboost.fit(X_train_sc, y_train) # 1. fit model on training data first\n",
    "\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']: # define a list of metrics we want computed on test\n",
    "        print(f'Train {metric}: {get_scorer(metric)(gs_gradboost, X_train_sc, y_train)}') # pass to get_scorer for metrics computation\n",
    "    print(f\"Train AUC = {get_scorer('roc_auc')(gs_gradboost, X_train_sc, y_train)}\")\n",
    "\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']: # define a list of metrics we want computed on test\n",
    "        print(f'Test {metric}: {get_scorer(metric)(gs_gradboost, X_test_sc, y_test)}') # pass to get_scorer for metrics computation\n",
    "    print(f\"Test AUC = {get_scorer('roc_auc')(gs_gradboost, X_test_sc, y_test)}\")\n",
    "    \n",
    "    return(gs_gradboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b336e04-2ef7-4c8a-b515-41aaa0741b33",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier Across Different Rolling Days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05175442-49c0-49bf-a1e7-c148218e6c2c",
   "metadata": {},
   "source": [
    "**(i) 10 Rolling Days Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "880155ea-12de-4f7c-80c6-fcd325bf62ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Train accuracy: 0.9492825685466686\n",
      "Train precision: 1.0\n",
      "Train recall: 0.032520325203252036\n",
      "Train f1: 0.06299212598425198\n",
      "Train AUC = 0.8781942362152257\n",
      "Test accuracy: 0.9475050475915777\n",
      "Test precision: 0.5\n",
      "Test recall: 0.01098901098901099\n",
      "Test f1: 0.021505376344086023\n",
      "Test AUC = 0.8394617559001121\n"
     ]
    }
   ],
   "source": [
    "gradboost_r10 = grad_boost_model(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d3dec9-f0ca-43de-b15b-8663e86d6d50",
   "metadata": {},
   "source": [
    "**(ii) 20 Rolling Days Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83a868f2-e686-4c88-a83a-63b952f626a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Train accuracy: 0.9495666998153147\n",
      "Train precision: 0.85\n",
      "Train recall: 0.04607046070460705\n",
      "Train f1: 0.08740359897172237\n",
      "Train AUC = 0.8734980070940139\n",
      "Test accuracy: 0.9480819152004615\n",
      "Test precision: 0.75\n",
      "Test recall: 0.016483516483516484\n",
      "Test f1: 0.03225806451612903\n",
      "Test AUC = 0.8358748557378695\n"
     ]
    }
   ],
   "source": [
    "gradboost_r20 = grad_boost_model(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9916ed-8f6d-4438-aaac-a99dce1e800f",
   "metadata": {},
   "source": [
    "**(iii) 30 Rolling Day Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98cde6ab-ca0f-4ef0-bf9b-572d3b02441e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Train accuracy: 0.9502770279869299\n",
      "Train precision: 0.7317073170731707\n",
      "Train recall: 0.08130081300813008\n",
      "Train f1: 0.14634146341463414\n",
      "Train AUC = 0.8812390552691134\n",
      "Test accuracy: 0.9489472166137871\n",
      "Test precision: 0.6666666666666666\n",
      "Test recall: 0.054945054945054944\n",
      "Test f1: 0.1015228426395939\n",
      "Test AUC = 0.8451126499071704\n"
     ]
    }
   ],
   "source": [
    "gradboost_r30 = grad_boost_model(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81939d12-b014-435b-9752-73076e5f67b1",
   "metadata": {},
   "source": [
    "**(iv) Baseline Model: Single Day Rolling Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70dce4d7-c222-4ce7-aa26-1768f110dd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Train accuracy: 0.9487143060093763\n",
      "Train precision: 0.8333333333333334\n",
      "Train recall: 0.02710027100271003\n",
      "Train f1: 0.05249343832020998\n",
      "Train AUC = 0.8673750523112428\n",
      "Test accuracy: 0.946928179982694\n",
      "Test precision: 0.0\n",
      "Test recall: 0.0\n",
      "Test f1: 0.0\n",
      "Test AUC = 0.8270225968856106\n"
     ]
    }
   ],
   "source": [
    "gradboost_r1 = grad_boost_model(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897737a-96bf-4a82-a131-2e2e41eccae2",
   "metadata": {},
   "source": [
    "The gradient boosting classifier achieved superior performance relative to the logistics regression, with the 30 days rolling average achieving a 0.881 Train AUC and 0.845 Test AUC.\n",
    "\n",
    "By now, it should be beyond disputing the rolling average model is far superior than a single day weather model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "781ed903-6de5-496c-ae95-6fc9f3c822f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradboost__learning_rate': 0.08,\n",
       " 'gradboost__max_depth': 3,\n",
       " 'gradboost__n_estimators': 50}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Examination of best parameters\n",
    "gradboost_r30.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05ff13c-bc55-4827-b0bb-2c1a198d63da",
   "metadata": {},
   "source": [
    "### (c) Light Gradient Boosting Machine Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d01ab-f9a3-4266-8489-783a6e78b5c1",
   "metadata": {},
   "source": [
    "We model using the Light Gradient Boosting Machine ([Ref.1](https://www.geeksforgeeks.org/lightgbm-light-gradient-boosting-machine/)) ([Ref.2](https://www.kaggle.com/code/prashant111/lightgbm-classifier-in-python)) next.  Theoretically, the light gradient boosting machine enhances upon the Gradient Boosting Classifier by incorporating features that (1) builds a lighter tree by only focusing on splits for under-trained variables, thereby maximising information gain in a \"lighter tree‚Äù, and (2) the bundling of sparse, uncorrelated variables, thereby allowing for faster convergence.  \n",
    "\n",
    "We conducting a gridsearch over different number of estimators, and learning rates. Likewise, we adopted measures to accommodate the highly imbalanced dataset, as well as Standard scaling for our weather-related variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f19d4a0b-dfc0-40b6-9c2d-99a503e6123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_boost_model(rolling_days, drop_codesum = False):\n",
    "    \n",
    "    # load in data\n",
    "    locals()['train_' + str(rolling_days)] = pd.read_csv('./assets/Modelling_Data/train_r'+ str(rolling_days) +'.csv', index_col=0)\n",
    "    \n",
    "    # drop Date Column\n",
    "    locals()['train_' + str(rolling_days)].drop(columns = ['Date'], inplace = True)\n",
    "    \n",
    "    # drop CodeSum Column\n",
    "    if drop_codesum == True:\n",
    "        locals()['train_' + str(rolling_days)].drop(columns = ['MIFG','TS','SQ','GR','VCFG','FG+','SN','FG',\n",
    "                                                               'VCTS','BCFG','BR','RA','FU','DZ','TSRA','HZ'], inplace = True)\n",
    "    \n",
    "    # Create X and Y\n",
    "    X = eval('train_' + str(rolling_days)).drop(columns = ['WnvPresent'])\n",
    "    X.reset_index(drop = True, inplace = True)\n",
    "    y = eval('train_' + str(rolling_days))['WnvPresent']\n",
    "    \n",
    "    # Dummify Columns\n",
    "    X = pd.get_dummies(X, columns=['Species', 'Trap'], drop_first = True)\n",
    "    \n",
    "    # Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 88, stratify = y)\n",
    "    \n",
    "    #Standard Scale weather variables\n",
    "    ct = ColumnTransformer([(\"sc\", StandardScaler(), \n",
    "                             ['Tavg', 'PrecipTotal','StnPressure','ResultDir','AvgSpeed','Sunlight'])],\n",
    "                          remainder = 'passthrough')\n",
    "\n",
    "    X_train_sc = ct.fit_transform(X_train)\n",
    "    X_test_sc = ct.transform(X_test)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    X_train_sc = pd.DataFrame(X_train_sc, columns=X_train.columns)\n",
    "    X_test_sc = pd.DataFrame(X_test_sc, columns=X_test.columns)\n",
    "    \n",
    "        \n",
    "    # Instantiate a pipeline\n",
    "    pipe_lgb = Pipeline([\n",
    "        ('lgb', lgb.LGBMClassifier(random_state = 88)) # tuple for estimator object, class\n",
    "    ])\n",
    "    \n",
    "    # Input Search Parameters!\n",
    "    pipe_lgb_params = {'lgb__n_estimators': [50, 75],\n",
    "                       'lgb__learning_rate': [.08, .1, .12],\n",
    "                       'lgb__max_depth': [3,5,10],\n",
    "                      }\n",
    "    \n",
    "    # Instantiate a Grid Search\n",
    "    gs_lgb = GridSearchCV(pipe_lgb, \n",
    "                          param_grid=pipe_lgb_params,\n",
    "                          cv=5,\n",
    "                          verbose=1,\n",
    "                          n_jobs=-1\n",
    "                         )\n",
    "\n",
    "    # Score!\n",
    "    gs_lgb.fit(X_train_sc, y_train) # 1. fit model on training data first\n",
    "\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']: # define a list of metrics we want computed on test\n",
    "        print(f'Train {metric}: {get_scorer(metric)(gs_lgb, X_train_sc, y_train)}') # pass to get_scorer for metrics computation\n",
    "    print(f\"Train AUC = {get_scorer('roc_auc')(gs_lgb, X_train_sc, y_train)}\")\n",
    "\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']: # define a list of metrics we want computed on test\n",
    "        print(f'Test {metric}: {get_scorer(metric)(gs_lgb, X_test_sc, y_test)}') # pass to get_scorer for metrics computation\n",
    "    print(f\"Test AUC = {get_scorer('roc_auc')(gs_lgb, X_test_sc, y_test)}\")\n",
    "    \n",
    "    return(gs_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a961bdf-2402-4da7-8251-a776aeb70656",
   "metadata": {},
   "source": [
    "#### Light Gradient Booster Across Different Rolling Days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48697bd7-a6c3-4605-ab99-893f2b99fcbc",
   "metadata": {},
   "source": [
    "**(i) 10 Rolling Days Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5b81551-8e8a-4560-a681-161e5a29d429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Train accuracy: 0.948003977837761\n",
      "Train precision: 1.0\n",
      "Train recall: 0.008130081300813009\n",
      "Train f1: 0.01612903225806452\n",
      "Train AUC = 0.907648817867489\n",
      "Test accuracy: 0.9475050475915777\n",
      "Test precision: 0.0\n",
      "Test recall: 0.0\n",
      "Test f1: 0.0\n",
      "Test AUC = 0.8596919062672488\n"
     ]
    }
   ],
   "source": [
    "lightboost_r10 = light_boost_model(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97099a-adbe-4eb3-aaec-1839b2cfab25",
   "metadata": {},
   "source": [
    "**(ii) 20 Rolling Days Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7108bd7-3351-4f84-9d60-2b0cbd845f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Train accuracy: 0.9488563716436994\n",
      "Train precision: 0.7647058823529411\n",
      "Train recall: 0.03523035230352303\n",
      "Train f1: 0.06735751295336788\n",
      "Train AUC = 0.914427745476855\n",
      "Test accuracy: 0.9477934813960196\n",
      "Test precision: 0.6\n",
      "Test recall: 0.016483516483516484\n",
      "Test f1: 0.0320855614973262\n",
      "Test AUC = 0.8462675832538846\n"
     ]
    }
   ],
   "source": [
    "lightboost_r20 = light_boost_model(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a826a0-55a3-4df2-a219-381ef7dadd77",
   "metadata": {},
   "source": [
    "**(iii) 30 Rolling Day Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1502bbef-3a01-4c24-9402-7bdaf4dcf71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Train accuracy: 0.9492825685466686\n",
      "Train precision: 0.8333333333333334\n",
      "Train recall: 0.04065040650406504\n",
      "Train f1: 0.07751937984496124\n",
      "Train AUC = 0.920021899619296\n",
      "Test accuracy: 0.9495240842226709\n",
      "Test precision: 1.0\n",
      "Test recall: 0.038461538461538464\n",
      "Test f1: 0.07407407407407407\n",
      "Test AUC = 0.8558833860203724\n"
     ]
    }
   ],
   "source": [
    "lightboost_r30 = light_boost_model(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802ef75-55e4-4018-86f2-8e01e24b1e45",
   "metadata": {},
   "source": [
    "**(iv) Baseline Model: Single Day Rolling Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d24685a-bad0-4b77-9126-6071e2cc043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Train accuracy: 0.9475777809347918\n",
      "Train precision: 0.0\n",
      "Train recall: 0.0\n",
      "Train f1: 0.0\n",
      "Train AUC = 0.863284008402303\n",
      "Test accuracy: 0.9475050475915777\n",
      "Test precision: 0.0\n",
      "Test recall: 0.0\n",
      "Test f1: 0.0\n",
      "Test AUC = 0.8352133406927927\n"
     ]
    }
   ],
   "source": [
    "lightboost_r1 = light_boost_model(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e8b3d-c5d2-4c4b-995a-5440fbd9a739",
   "metadata": {},
   "source": [
    "The gradient boosting classifier achieved seemingly achieved superior performance relative to both the logistics regression and gradient boosting classifier.  **The 30 days rolling average achieved the best results of all with a 0.918 Train AUC and a 0.852 Test AUC.**\n",
    "\n",
    "However, we note that the light gradient boosting model produces instances of exceptionally severe overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48951030-a20b-47e3-9a28-f97ee9df3e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lgb__learning_rate': 0.08, 'lgb__max_depth': 5, 'lgb__n_estimators': 50}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Examination of best parameters\n",
    "lightboost_r10.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fde5be-3985-4830-8ed5-0136cce46625",
   "metadata": {},
   "source": [
    "# (2) Choosing a Production Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16febf94-6af3-4d3c-9065-2f5c464a3719",
   "metadata": {},
   "source": [
    "Following a Gridsearch for the optimal parameters of our candidate models, we ascertained that the Light Gradient Boosting Model achieved the best results. However, we are mindful of the following:\n",
    "\n",
    "(1) **The performance of the light boosting gradient method was only marginally better**. This also came at a cost of much more severe overfitting by the light gradient boosting model as opposed to say, the Logistics Regression Model. Consequently, the light gradient boosting model might not fit as well on new, unseen data.\n",
    "\n",
    "(2) **Among the candidate models, the logistics regression model provides best \"white box\" explanability**. A non-technical audience can better sieve out key factors leading to WNV onset, and correspondingly adopt measures to tackle the virus. This model will ultimately be more palatable to the non-technical city management. \n",
    "\n",
    "With the above consideration, **we deem the logistics regression model to be most suitable for production.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d6f7e-0ed4-4bfa-b77e-2c6a66f977cd",
   "metadata": {},
   "source": [
    "## ROC Curve for Chosen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d33885b-e346-4a29-bbf9-e1dc7bbaf542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_model_selected(rolling_days, drop_codesum = False):\n",
    "    \n",
    "    # load in data\n",
    "    locals()['train_' + str(rolling_days)] = pd.read_csv('./assets/Modelling_Data/train_r'+ str(rolling_days) +'.csv', index_col=0)\n",
    "    \n",
    "    # drop Date Column\n",
    "    locals()['train_' + str(rolling_days)].drop(columns = ['Date'], inplace = True)\n",
    "    \n",
    "    # drop CodeSum Column\n",
    "    if drop_codesum == True:\n",
    "        locals()['train_' + str(rolling_days)].drop(columns = ['MIFG','TS','SQ','GR','VCFG','FG+','SN','FG',\n",
    "                                                               'VCTS','BCFG','BR','RA','FU','DZ','TSRA','HZ'], inplace = True)\n",
    "    \n",
    "    # Create X and Y\n",
    "    X = eval('train_' + str(rolling_days)).drop(columns = ['WnvPresent'])\n",
    "    X.reset_index(drop = True, inplace = True)\n",
    "    y = eval('train_' + str(rolling_days))['WnvPresent']\n",
    "    \n",
    "    # Dummify Columns\n",
    "    X = pd.get_dummies(X, columns=['Species', 'Trap'], drop_first = True)\n",
    "    \n",
    "    # Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 88, stratify = y)\n",
    "    \n",
    "    #Standard Scale weather variables\n",
    "    ct = ColumnTransformer([(\"sc\", StandardScaler(), \n",
    "                             ['Tavg', 'PrecipTotal','StnPressure','ResultDir','AvgSpeed','Sunlight'])],\n",
    "                           remainder = 'passthrough')\n",
    "\n",
    "    X_train_sc = ct.fit_transform(X_train)\n",
    "    X_test_sc = ct.transform(X_test)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    X_train_sc = pd.DataFrame(X_train_sc, columns=X_train.columns)\n",
    "    X_test_sc = pd.DataFrame(X_test_sc, columns=X_test.columns)\n",
    "    \n",
    "    # Instantiate a pipeline\n",
    "    logreg = LogisticRegression(class_weight='balanced', solver = 'saga', max_iter= 10000, C = 1000)\n",
    "\n",
    "    # Score!\n",
    "    logreg.fit(X_train_sc, y_train) # 1. fit model on training data first\n",
    "    print(f\"Test AUC = {get_scorer('roc_auc')(logreg, X_test_sc, y_test)}\")\n",
    "    \n",
    "    return(logreg, X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "377d21ef-122c-49c9-bf92-df4535554c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC = 0.8280872095940589\n"
     ]
    }
   ],
   "source": [
    "model, X_test, y_test= log_reg_model_selected(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8209a977-8668-4d5b-b7e8-28e09c232331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_curve(model, X_test, y_test)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704fef5-fcce-4fce-ae97-cda655493b00",
   "metadata": {},
   "source": [
    "## Logistic Regression Function for Final Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27984c-f1d3-492f-8e7c-36e596802eb5",
   "metadata": {},
   "source": [
    "To set our chosen model for production, we retrain the Logistic Regression model using the entire train set, before publishing our predicted values into a separate csv file.\n",
    "\n",
    "**Submitting our results to Kaggle, where we achieved an AUC score of 0.751.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95e6d954-1ef6-4b61-a8f6-43c65a993c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logregger(rolling_days, drop_codesum = False):\n",
    "    \n",
    "    \n",
    "    # Obtain Data Set\n",
    "    train = pd.read_csv('./assets/Modelling_Data/train_r' + str(rolling_days) +'.csv', index_col=0)\n",
    "    test = pd.read_csv('./assets/Modelling_Data/test_r' + str(rolling_days) +'.csv', index_col=0)\n",
    "    \n",
    "    # Drop Date Column\n",
    "    train.drop(columns = ['Date'], inplace = True)\n",
    "    test.drop(columns = ['Date', 'Id'], inplace = True)\n",
    "    \n",
    "    # drop CodeSum Column\n",
    "    if drop_codesum == True:\n",
    "        train.drop(columns = ['MIFG','TS','SQ','GR','VCFG','FG+','SN','FG',\n",
    "                              'VCTS','BCFG','BR','RA','FU','DZ','TSRA','HZ'], inplace = True)\n",
    "        test.drop(columns = ['MIFG','TS','SQ','GR','VCFG','FG+','SN','FG',\n",
    "                              'VCTS','BCFG','BR','RA','FU','DZ','TSRA','HZ'], inplace = True)\n",
    "        \n",
    "    \n",
    "    # Create X and Y\n",
    "    X_train = train.drop(columns = ['WnvPresent'])\n",
    "    y_train = train['WnvPresent']\n",
    "    \n",
    "    # Dummify Columns\n",
    "    X_train_dummy = pd.get_dummies(X_train, columns=['Species', 'Trap'], drop_first = True)\n",
    "    X_test_dummy = pd.get_dummies(test, columns=['Species', 'Trap'], drop_first = True)\n",
    "    \n",
    "    # Standard Scale weather variables\n",
    "    ct = ColumnTransformer([(\"sc\", StandardScaler(), \n",
    "                         ['Tavg', 'PrecipTotal','StnPressure','ResultDir','AvgSpeed','Sunlight'])],\n",
    "                      remainder = 'passthrough')\n",
    "\n",
    "    X_train_dummy_sc = ct.fit_transform(X_train_dummy)\n",
    "    X_test_dummy_sc = ct.transform(X_test_dummy)\n",
    "    \n",
    "    # Convert back to data frame with columns\n",
    "    X_train_final = pd.DataFrame(X_train_dummy_sc, columns=X_train_dummy.columns)\n",
    "    X_test_final = pd.DataFrame(X_test_dummy_sc, columns=X_train_dummy.columns)\n",
    "    \n",
    "    # Fit model on train\n",
    "    logreg = LogisticRegression(class_weight='balanced', solver = 'saga', max_iter = 40000, C = 1000)\n",
    "    logreg.fit(X_train_final,y_train)\n",
    "    \n",
    "    # Prepare dataframe\n",
    "    answer = pd.DataFrame(logreg.predict_proba(X_test_final))\n",
    "    answer.drop(columns =[0], inplace = True)\n",
    "    answer = pd.concat([pd.DataFrame(np.linspace(1,len(answer),num=len(answer))), answer],axis = 1)\n",
    "    answer.rename(mapper= {0:'Id',1:'WnvPresent'}, axis = 1, inplace = True)\n",
    "    answer['Id'] = answer['Id'].astype(np.int64)\n",
    "    \n",
    "    # Export file to submissions folder\n",
    "    if drop_codesum == True:\n",
    "        answer.to_csv('./assets/Submissions/r' + str(rolling_days) +'_lr_dropped.csv', index = False)\n",
    "    else: \n",
    "        answer.to_csv('./assets/Submissions/r' + str(rolling_days) +'_lr.csv', index = False)\n",
    "        \n",
    "    return((logreg,X_train_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eda1de9d-cb00-4243-8fb1-68386adfa972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Output of Model\n",
    "results = logregger(30)\n",
    "production_model = results[0]\n",
    "X_train_final = results[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de0b23-56ca-4662-bd3e-b77d5722d4a3",
   "metadata": {},
   "source": [
    "# (3) Examination of Most Important Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5756e1c-437b-4219-950b-65a6b8bbd5c6",
   "metadata": {},
   "source": [
    "Examining the most important variables, we discover the following to impact the onset of WNV most significantly:\n",
    "\n",
    "(1) **Precipitation (Most Positive)**. Presence of rain / drizzle casts the greatest impact in causing WNV onset.\n",
    "\n",
    "(2) **Fogs and Smoke (Most Negative)**. Meanwhile, the presence of Mist / Fog / Smoke deters the onset of WNV most significantly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98a92561-a267-47b7-bdce-a80909054251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine most important variables\n",
    "coefficients = pd.DataFrame(production_model.coef_, columns = X_train_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "132487ff-9b6b-4a31-8cc4-1f281105893b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCFG    44.345991\n",
       "DZ      10.853193\n",
       "RA       6.740013\n",
       "FG       6.492740\n",
       "VCTS     5.513570\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Variables with Highest Coefficients (Positive)\n",
    "(coefficients.T)[0].sort_values(ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "903b27bc-416f-4cd0-8ff0-5b2e7385e49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VCFG   -78.814172\n",
       "SQ     -47.459589\n",
       "GR     -27.759586\n",
       "FG+    -25.778627\n",
       "MIFG    -8.802805\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Variables with Highest Coefficients (Negative)\n",
    "(coefficients.T)[0].sort_values(ascending=True)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d9382-a997-43ae-8882-278baabecfed",
   "metadata": {},
   "source": [
    "# (4) Further Exploration: The Case for Higher Rolling Days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3cb0fc-99b6-45cb-8dca-a386b6f13223",
   "metadata": {},
   "source": [
    "We have earlier experimented with rolling weather averages of 10, 20 and 30 days for our model. The key idea is these number of days should adequately account for the duration of optimal weather required for mosquito eggs to \"unhibernate\", to hatch, and for the adults to become sufficiently active. However, while we can scientifically establish that it takes 8 to 10 days for a hatched mosquito egg to become an adult, the science behind how animals regulate themselves to wake up from hibernation is more abstract.\n",
    "\n",
    "By some accounts, animal wakes up from hibernation using some combination of [internal body clock](https://earthsky.org/earth/how-do-hibernating-animals-know-when-to-wake-up/) and [external weather stimulus](https://www.grunge.com/1055546/how-do-hibernating-animals-know-when-to-wake-up/). Of note, waking up from hibernation is a massive endeavour for most animals requiring them to turn on many calorie-burning bodily functions - waking up too early can spell doom for hibernating animals. The point is, animals may take even longer-term weather averages into account when deciding to come out of hibernation. It is therefore not unthinkable that even rolling day averages beyond 30 days could be a key factor when determining mosquito activity. \n",
    "\n",
    "In a exploratory instance, we used a rolling average of 80 days to train a logistics model (here we drop the CodeSum variables since we opine that these variables loses their significance over so many days).\n",
    "\n",
    "**Submitting the results to Kaggle, we obtain an AUC score of 0.790**, which is slightly superior to our previous results. Is this due to sheer luck? The hibernation behaviour of mosquito eggs is certainly worth further examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5165c487-a4f2-4c27-9967-2de8efe76a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying for more outlandish days...\n",
    "logregger_80 = logregger(80, drop_codesum = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b30b5-41ad-454e-9d16-470e862a4356",
   "metadata": {},
   "source": [
    "![image](https://yohta-blog.yokohama-oht.com/hs-fs/hubfs/well-its-groundhog-day-again.jpg?width=1000&height=720&name=well-its-groundhog-day-again.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ea71e4-b3ba-4f1c-b325-b2c35bb56b45",
   "metadata": {},
   "source": [
    "# (5) Bonus: Logistics Regression with SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1f315-c4f4-469c-810c-850a60d064cc",
   "metadata": {},
   "source": [
    "We took the liberty to explore an alternate method [Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/). This technique synthesises the under-represented observations (in this case,'WnvPresent' = 1) by applying KNN on the observations to create more data points, towards creating a more balanced dataset. Theoretically, this prevents an \"over-learning\" of the the limited positive cases we have, thereby mitigating any overfit. However, it also [documented](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjl-u64k_z6AhWNm9gFHQeID3sQFnoECBUQAw&url=https%3A%2F%2Fwww.mdpi.com%2F2073-8994%2F13%2F2%2F194%2Fpdf&usg=AOvVaw00Iv0stQnUPpsP4dEGfoUF) that the model oversamples noises (which in turn aggravates overfitting).\n",
    "\n",
    "The method produced slightly superior results. However, the more severe overfitting leads us to decide against using the model for our final production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28913443-0c8e-4527-8e09-524a8d7abd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf139c-34a2-4a79-b5e9-8a95c6db218c",
   "metadata": {},
   "source": [
    "### Function for SMOTE-Enabled Logistic Regression Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb8490d8-ca29-4840-9943-e2d7100a7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_model_SMOTE(rolling_days, drop_codesum = False):\n",
    "    \n",
    "    print(f\"LOG REG with SMOTE: {rolling_days} days rolling weather.\") \n",
    "    \n",
    "    # load in data\n",
    "    locals()['train_' + str(rolling_days)] = pd.read_csv('./assets/Modelling_Data/train_r'+ str(rolling_days) +'.csv', index_col=0)\n",
    "    \n",
    "    # drop Date Column\n",
    "    locals()['train_' + str(rolling_days)].drop(columns = ['Date'], inplace = True)\n",
    "    \n",
    "    # drop CodeSum Column\n",
    "    if drop_codesum == True:\n",
    "        locals()['train_' + str(rolling_days)].drop(columns = ['MIFG','TS','SQ','GR','VCFG','FG+','SN','FG',\n",
    "                                                               'VCTS','BCFG','BR','RA','FU','DZ','TSRA','HZ'], inplace = True)\n",
    "        \n",
    "    # Create X and Y\n",
    "    X = eval('train_' + str(rolling_days)).drop(columns = ['WnvPresent'])\n",
    "    X.reset_index(drop = True, inplace = True)\n",
    "    y = eval('train_' + str(rolling_days))['WnvPresent']\n",
    "    \n",
    "    # Dummify Columns\n",
    "    X = pd.get_dummies(X, columns=['Species', 'Trap'], drop_first = True)\n",
    "    \n",
    "    # Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 88, stratify = y)\n",
    "    \n",
    "    # Standard Scale weather variables\n",
    "    ct = ColumnTransformer([(\"sc\", StandardScaler(), \n",
    "                             ['Tavg', 'PrecipTotal','StnPressure','ResultDir','AvgSpeed','Sunlight'])],\n",
    "                          remainder = 'passthrough')\n",
    "\n",
    "    X_train_sc = ct.fit_transform(X_train)\n",
    "    X_test_sc = ct.transform(X_test)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    X_train_sc = pd.DataFrame(X_train_sc, columns=X_train.columns)\n",
    "    X_test_sc = pd.DataFrame(X_test_sc, columns=X_test.columns)\n",
    "    \n",
    "    # SMOTE HERE\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train_sc, y_train = sm.fit_resample(X_train_sc, y_train)\n",
    "    \n",
    "    # Instantiate a pipeline\n",
    "    pipe_logreg = Pipeline([\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', solver = 'saga', max_iter=10000)) # tuple for estimator object, class\n",
    "    ])\n",
    "    \n",
    "    # Input Search Parameters!\n",
    "    pipe_logreg_params = {'logreg__C': np.logspace(-3,3,7)}\n",
    "    \n",
    "    # Instantiate a Grid Search\n",
    "    gs_logreg = GridSearchCV(pipe_logreg, \n",
    "                             param_grid=pipe_logreg_params,\n",
    "                             cv=5,\n",
    "                             verbose=1,\n",
    "                             n_jobs=-1\n",
    "                            )\n",
    "    \n",
    "    # Score!\n",
    "    \n",
    "    gs_logreg.fit(X_train_sc, y_train) # 1. fit model on training data first\n",
    "\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']: # define a list of metrics we want computed on test\n",
    "        print(f'Train {metric}: {get_scorer(metric)(gs_logreg, X_train_sc, y_train)}') # pass to get_scorer for metrics computation\n",
    "    print(f\"Train AUC = {get_scorer('roc_auc')(gs_logreg, X_train_sc, y_train)}\")\n",
    "\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']: # define a list of metrics we want computed on test\n",
    "        print(f'Test {metric}: {get_scorer(metric)(gs_logreg, X_test_sc, y_test)}') # pass to get_scorer for metrics computation\n",
    "    print(f\"Test AUC = {get_scorer('roc_auc')(gs_logreg, X_test_sc, y_test)}\")\n",
    "    \n",
    "    print('**************************************************')    \n",
    "        \n",
    "    return(gs_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8253079f-332f-4256-9e6f-8405357010ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG REG with SMOTE: 30 days rolling weather.\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Train accuracy: 0.818215892053973\n",
      "Train precision: 0.785704670884372\n",
      "Train recall: 0.8751124437781109\n",
      "Train f1: 0.8280019859564508\n",
      "Train AUC = 0.8860695364461696\n",
      "Test accuracy: 0.7683876550331699\n",
      "Test precision: 0.1569060773480663\n",
      "Test recall: 0.7802197802197802\n",
      "Test f1: 0.2612695492180313\n",
      "Test AUC = 0.8272575978055432\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "logreg_smote_r30 = log_reg_model_SMOTE(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a61a7-6dc6-4a8f-90fe-25919493ca0c",
   "metadata": {},
   "source": [
    "### Function for SMOTE-Enabled Logistic Regression Submission (Not Used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "886663d6-16c1-4ba5-b415-5ada07a76247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logregger_smote(rolling_days, drop_codesum = False):\n",
    "    \n",
    "    # Obtain Data Set\n",
    "    train = pd.read_csv('./assets/Modelling_Data/train_r' + str(rolling_days) +'.csv', index_col=0)\n",
    "    test = pd.read_csv('./assets/Modelling_Data/test_r' + str(rolling_days) +'.csv', index_col=0)\n",
    "    \n",
    "    # Drop Date Column\n",
    "    train.drop(columns = ['Date'], inplace = True)\n",
    "    test.drop(columns = ['Date', 'Id'], inplace = True)\n",
    "    \n",
    "    # drop CodeSum Column\n",
    "    if drop_codesum == True:\n",
    "        train.drop(columns = ['MIFG','TS','SQ','GR','VCFG','FG+','SN','FG',\n",
    "                              'VCTS','BCFG','BR','RA','FU','DZ','TSRA','HZ'], inplace = True)\n",
    "        test.drop(columns = ['MIFG','TS','SQ','GR','VCFG','FG+','SN','FG',\n",
    "                              'VCTS','BCFG','BR','RA','FU','DZ','TSRA','HZ'], inplace = True)    \n",
    "    \n",
    "    # Create X and Y\n",
    "    X_train = train.drop(columns = ['WnvPresent'])\n",
    "    y_train = train['WnvPresent']\n",
    "    \n",
    "    # Dummify Columns\n",
    "    X_train_dummy = pd.get_dummies(X_train, columns=['Species', 'Trap'], drop_first = True)\n",
    "    X_test_dummy = pd.get_dummies(test, columns=['Species', 'Trap'], drop_first = True)\n",
    "    \n",
    "    # Standard Scale weather variables\n",
    "    ct = ColumnTransformer([(\"sc\", StandardScaler(), \n",
    "                         ['Tavg', 'PrecipTotal','StnPressure','ResultDir','AvgSpeed','Sunlight'])],\n",
    "                      remainder = 'passthrough')\n",
    "\n",
    "    X_train_dummy_sc = ct.fit_transform(X_train_dummy)\n",
    "    X_test_dummy_sc = ct.transform(X_test_dummy)\n",
    "    \n",
    "    # Convert back to data frame with columns\n",
    "    X_train_final = pd.DataFrame(X_train_dummy_sc, columns=X_train_dummy.columns)\n",
    "    X_test_final = pd.DataFrame(X_test_dummy_sc, columns=X_train_dummy.columns)\n",
    "    \n",
    "    # Apply SMOTE to expand data set\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_train_final, y_train)\n",
    "    \n",
    "    # Fit model on train\n",
    "    logreg = LogisticRegression(penalty = 'l2', solver = 'sag', C = 1.0, max_iter=1000)\n",
    "    logreg.fit(X_res,y_res)\n",
    "    \n",
    "    # Prepare dataframe\n",
    "    answer = pd.DataFrame(logreg.predict_proba(X_test_final))\n",
    "    answer.drop(columns =[0], inplace = True)\n",
    "    answer = pd.concat([pd.DataFrame(np.linspace(1,len(answer),num=len(answer))), answer],axis = 1)\n",
    "    answer.rename(mapper= {0:'Id',1:'WnvPresent'}, axis = 1, inplace = True)\n",
    "    answer['Id'] = answer['Id'].astype(np.int64)\n",
    "    \n",
    "    # Export file to submissions folder\n",
    "    if drop_codesum == True:\n",
    "        answer.to_csv('./assets/Submissions/r' + str(rolling_days) +'_lr_dropped_smote.csv', index = False)\n",
    "    else: \n",
    "        answer.to_csv('./assets/Submissions/r' + str(rolling_days) +'_lr_smote.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a384f6f-e073-4f7e-89ec-32f975db6358",
   "metadata": {},
   "source": [
    "# (6) Bonus: Neural Nets Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc77406-68f5-4121-8b42-823346d08939",
   "metadata": {},
   "source": [
    "We experimented with the implementation of a neural net for WNV prediction. With the caveat that we did not perform a train set cross-validation nor did we finetune the model, we were not able to obtain a better performance than what we already have (AUC of 0.73 when submitted to Kaggle). Concurrently, the model displayed difficulties in converging, and we opine that it is best to not use it for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad030ce5-f775-457d-9c4d-9bb8f13d32ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98c5cca4-ce0d-44e0-b467-1b30fcfc6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_nets(rolling_days, drop_codesum = False):\n",
    "    \n",
    "    # Obtain Data Set\n",
    "    train = pd.read_csv('./assets/Modelling_Data/train_r' + str(rolling_days) +'.csv', index_col=0)\n",
    "    test = pd.read_csv('./assets/Modelling_Data/test_r' + str(rolling_days) +'.csv', index_col=0)\n",
    "    \n",
    "    # Drop Date Column\n",
    "    train.drop(columns = ['Date'], inplace = True)\n",
    "    test.drop(columns = ['Date', 'Id'], inplace = True)\n",
    "    \n",
    "    # drop CodeSum Column\n",
    "    if drop_codesum == True:\n",
    "        train.drop(columns = ['MIFG','TS','SQ','GR','VCFG','FG+','SN','FG',\n",
    "                              'VCTS','BCFG','BR','RA','FU','DZ','TSRA','HZ'], inplace = True)\n",
    "        test.drop(columns = ['MIFG','TS','SQ','GR','VCFG','FG+','SN','FG',\n",
    "                              'VCTS','BCFG','BR','RA','FU','DZ','TSRA','HZ'], inplace = True)\n",
    "        \n",
    "    \n",
    "    # Create X and Y\n",
    "    X_train = train.drop(columns = ['WnvPresent'])\n",
    "    y_train = train['WnvPresent']\n",
    "    \n",
    "    # Dummify Columns\n",
    "    X_train_dummy = pd.get_dummies(X_train, columns=['Species', 'Trap'], drop_first = True)\n",
    "    X_test_dummy = pd.get_dummies(test, columns=['Species', 'Trap'], drop_first = True)\n",
    "    \n",
    "    # Standard Scale weather variables\n",
    "    ct = ColumnTransformer([(\"sc\", StandardScaler(), \n",
    "                         ['Tavg', 'PrecipTotal','StnPressure','ResultDir','AvgSpeed','Sunlight'])],\n",
    "                      remainder = 'passthrough')\n",
    "\n",
    "    X_train_dummy_sc = ct.fit_transform(X_train_dummy)\n",
    "    X_test_dummy_sc = ct.transform(X_test_dummy)\n",
    "    \n",
    "    # Convert back to data frame with columns\n",
    "    X_train_final = pd.DataFrame(X_train_dummy_sc, columns=X_train_dummy.columns)\n",
    "    X_test_final = pd.DataFrame(X_test_dummy_sc, columns=X_train_dummy.columns)\n",
    "    \n",
    "    # Fit model on train\n",
    "    nn = MLPClassifier(hidden_layer_sizes = (50,100,150,100,50), \n",
    "                       random_state = 88, \n",
    "                       learning_rate = 'adaptive',\n",
    "                       alpha = 0.05\n",
    "                      )\n",
    "    nn.fit(X_train_final,y_train)\n",
    "    \n",
    "    # Prepare dataframe\n",
    "    answer = pd.DataFrame(nn.predict_proba(X_test_final))\n",
    "    answer.drop(columns =[0], inplace = True)\n",
    "    answer = pd.concat([pd.DataFrame(np.linspace(1,len(answer),num=len(answer))), answer],axis = 1)\n",
    "    answer.rename(mapper= {0:'Id',1:'WnvPresent'}, axis = 1, inplace = True)\n",
    "    answer['Id'] = answer['Id'].astype(np.int64)\n",
    "    \n",
    "    # Export file to submissions folder\n",
    "    if drop_codesum == True:\n",
    "        answer.to_csv('./assets/Submissions/r' + str(rolling_days) +'_nn_dropped.csv', index = False)\n",
    "    else: \n",
    "        answer.to_csv('./assets/Submissions/r' + str(rolling_days) +'_nn.csv', index = False)\n",
    "        \n",
    "    return((nn,X_train_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "574f6620-7949-493b-bf2f-141d41fe4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "nn_model = neural_nets(30, drop_codesum = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
